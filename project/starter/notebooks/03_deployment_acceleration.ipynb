{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciMed | Notebook 3: Hardware Acceleration & Production Deployment\n",
    "\n",
    "Welcome to the final phase of UdaciMed's optimization pipeline! In this notebook, you will implement cross-platform hardware acceleration techniques and strategize for the deployment of your optimized model across hardware targets.\n",
    "\n",
    "## Recap: Optimization Journey\n",
    "\n",
    "In [Notebook 2](02_architecture_optimization.ipynb), you have implemented architectural optimizations that brought you closer to your optimization targets.\n",
    "\n",
    "Now, it is time to unlock further performance opportunities with hardware acceleration.\n",
    "\n",
    "> **Your mission**: Transform your optimized model into a production-ready cross-platform deployment that meets production SLAs on this reference hardware, and finalize UdaciMed's deployment strategy across its diverse hardware fleet.\n",
    "\n",
    "### Hardware acceleration\n",
    "\n",
    "You will implement and evaluate **2 core deployment techniques\\*** using [ONNX Runtime](https://onnxruntime.ai/):\n",
    "\n",
    "1. **Mixed Precision (FP16)** - Utilizing 16-bit floating-point numbers to significantly speed up calculations and reduce memory usage on compatible hardware.\n",
    "2. **Dynamic Batching** - Finding the best batch size to maximize throughput for offline tasks while maintaining low latency for real-time requests.\n",
    "\n",
    "Additionally, you will analyze three deployment scenarios: GPU (TensorRT), CPU (OpenVINO), and Edge deployment considerations.\n",
    "\n",
    "_\\* Note that while you are expected to implement both deployment techniques, you can decide whether to keep either or both in your final deployment strategy to best achieve targets._\n",
    "\n",
    "---\n",
    "\n",
    "Through this notebook, you will:\n",
    "\n",
    "- **Convert PyTorch model to ONNX** for cross-platform deployment\n",
    "- **Apply hardware acceleration using ONNX Runtime** on the reference T4 device\n",
    "- **Benchmark end-to-end performance** against SLAs\n",
    "- **Validate clinical safety** across the deployment pipeline\n",
    "- **Analyze alternative deployment strategies** for diverse hardware environments\n",
    "\n",
    "**Let's deliver a production-ready, hardware-accelerated diagnostic deployment!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup the environment\n",
    "\n",
    "First, let's set up the environment and understand our reference hardware capabilities. \n",
    "\n",
    "This ensures our optimization and benchmarking code will run smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any, Literal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project utilities\n",
    "from utils.data_loader import (\n",
    "    load_pneumoniamnist,\n",
    "    get_sample_batch\n",
    ")\n",
    "from utils.model import (\n",
    "    create_baseline_model,\n",
    "    get_model_info\n",
    ")\n",
    "from utils.evaluation import (\n",
    "    evaluate_with_multiple_thresholds\n",
    ")\n",
    "from utils.profiling import (\n",
    "    PerformanceProfiler,\n",
    "    measure_time\n",
    ")\n",
    "from utils.visualization import (\n",
    "    plot_performance_profile,\n",
    "    plot_batch_size_comparison\n",
    ")\n",
    "from utils.architecture_optimization import (\n",
    "    create_optimized_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 14.6 GB\n",
      "Tensor Core Support: True\n",
      "Default hardware acceleration environment ready!\n",
      "\n",
      "ONNX Runtime available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# Set device and analyze hardware capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Check tensor core support for mixed precision - crucial for FP16 acceleration\n",
    "    gpu_compute = torch.cuda.get_device_properties(0).major\n",
    "    tensor_core_support = gpu_compute >= 7  # Volta+ architecture\n",
    "    print(f\"Tensor Core Support: {tensor_core_support}\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available - hardware acceleration will be limited\")\n",
    "\n",
    "print(\"Default hardware acceleration environment ready!\")\n",
    "\n",
    "# Verify ONNX Runtime GPU support\n",
    "print(f\"\\nONNX Runtime available providers: {ort.get_available_providers()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Getting ready for acceleration**: The checks above highlight two critical facts for our mission:\n",
    "> 1. Our reference hardware has tensor core support, which can dramatically speed up 16-bit floating-point (FP16) calculations; for other hardware deployments, like CPUs that lack this feature, we would need to rely on different techniques (such as 8-bit integer quantization (INT8)) to achieve similar acceleration.\n",
    "> 2. ONNX Runtime providers are available for our primary targets: CUDAExecutionProvider for GPU and CPUExecutionProvider for CPU. This allows us to benchmark on both platforms. For a true mobile or edge deployment, we would need to use a specialized package like ONNX Runtime Mobile, which is built separately to keep the application lightweight.\n",
    "> \n",
    "> Our task is to meet SLAs on our current device, which means we must **_benchmark against the GPU_** to see if we've met our goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load test data and optimized model with configuration\n",
    "\n",
    "The model is needed for deployment, and the optimization results for comparison.\n",
    "\n",
    "Test data is needed for both conversion and final performance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /voc/work/.medmnist/pneumoniamnist_64.npz\n",
      "Test data loaded: torch.Size([32, 3, 64, 64]) batch for hardware acceleration profiling\n"
     ]
    }
   ],
   "source": [
    "# Define dataset loading parameters\n",
    "img_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Load test dataset for final evaluation\n",
    "test_loader = load_pneumoniamnist(\n",
    "    split=\"test\", \n",
    "    download=True, \n",
    "    size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=None\n",
    ")\n",
    "\n",
    "# Get sample batch for profiling\n",
    "sample_images, sample_labels = get_sample_batch(test_loader)\n",
    "sample_images = sample_images.to(device)\n",
    "sample_labels = sample_labels.to(device)\n",
    "\n",
    "print(f\"Test data loaded: {sample_images.shape} batch for hardware acceleration profiling\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Batch size strategy**: Your batch size choice impacts memory usage, latency, and throughput. \n",
    "> \n",
    "> Consider: What batch size best applied for each deployment scenario? Don't forget to review the batch analysis plot from Notebook 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded optimization results from Notebook 2:\n",
      "   Model: ResNet-18 Optimized\n",
      "   Clinical Performance: 98.7% sensitivity\n",
      "   Architecture Speedup: 0.76x\n",
      "   Memory Reduction: 73.3%\n"
     ]
    }
   ],
   "source": [
    "# Load optimized model and results from notebook 2\n",
    "\n",
    "# TODO: Define the experiment name\n",
    "experiment_name = \"interpolation-removal_depthwise-separable_channels-last\"# String - Add your value here\n",
    "\n",
    "with open(f'./results/optimization_results_{experiment_name}.pkl', 'rb') as f:\n",
    "    optimization_results = pickle.load(f)\n",
    "\n",
    "print(\"Loaded optimization results from Notebook 2:\")\n",
    "print(f\"   Model: {optimization_results['model_name']}\")\n",
    "print(f\"   Clinical Performance: {optimization_results['clinical_performance']['optimized']['sensitivity']:.1%} sensitivity\")\n",
    "print(f\"   Architecture Speedup: {optimization_results['performance_improvements']['latency_speedup']:.2f}x\")\n",
    "print(f\"   Memory Reduction: {optimization_results['performance_improvements']['memory_reduction_percent']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **HINT: Finding your optimization results**\n",
    "> \n",
    "> Your optimization results from Notebook 2 should be saved as:\n",
    "> - Results file: `../results/optimization_results_{experiment_name}.pkl`\n",
    "> - Model weights: `../results/optimized_model.pth`\n",
    "> \n",
    "> The experiment name typically combines your optimization techniques, like:\n",
    "> - `\"interpolation-removal_depthwise-separable\"`\n",
    "> - `\"channel-reduction_grouped-conv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting clinical model optimization pipeline...\n",
      "   Applying interpolation removal optimization...\n",
      "Applying native resolution optimization (64x64)...\n",
      "INTERPOLATION REMOVAL completed.\n",
      "   Applying depthwise separable optimization...\n",
      "DEPTHWISE SEPARABLE completed: 16 replacements\n",
      "Applied optimizations: interpolation_removal → depthwise_separable\n"
     ]
    }
   ],
   "source": [
    "# Get the optimization configuration\n",
    "opt_config = optimization_results['optimization_config']\n",
    "base_model = create_baseline_model(\n",
    "    num_classes=2,\n",
    "    input_size=64,\n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "# Apply the same architectural modifications\n",
    "optimized_model = create_optimized_model(base_model, opt_config)\n",
    "\n",
    "# Load the trained weights\n",
    "optimized_model.load_state_dict(\n",
    "    torch.load('./results/optimized_model.pth', map_location=device)\n",
    ")\n",
    "optimized_model = optimized_model.to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert model with hardware acceleration for production deployment\n",
    "\n",
    "Convert the optimized model to [ONNX (Open Neural Network Exchange)](https://onnx.ai/) with optional hardware accelerations. \n",
    "\n",
    "**IMPORTANT**: You are tasked to implement both hardware optimizations even if you decide to disable them for the final export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your deployment configuration for the ONNX export.\n",
    "# GOAL: Decide whether to use mixed precision (FP16) and/or dynamic batching for the final export.\n",
    "# HINT: Setting use_fp16 to True can significantly improve performance on compatible GPUs (like the T4 with Tensor Cores)\n",
    "# but may introduce a minor, often negligible, loss in precision. We'll validate the clinical impact later.\n",
    "\n",
    "use_fp16 = True  # Enable mixed precision for T4 GPU with Tensor Cores\n",
    "use_dynamic_batching = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to ONNX format...\n",
      "   Input shape: torch.Size([32, 3, 64, 64])\n",
      "   Input dtype: torch.float16\n",
      "   FP16 mode: True\n",
      "   Export path: ./results/onnx_models/udacimed_pneumonia_optimized.onnx\n",
      "ONNX export completed: ./results/onnx_models/udacimed_pneumonia_optimized.onnx\n",
      "   ONNX model verification passed\n"
     ]
    }
   ],
   "source": [
    "# Convert PyTorch model to ONNX format (for cross-platform deployment)\n",
    "\n",
    "def export_model_to_onnx(model: nn.Module, input_tensor: torch.Tensor, \n",
    "                        export_path: str, model_name: str = \"pneumonia_detection\", \n",
    "                        fp16_mode: bool = use_fp16, dynamic_batching: bool = use_dynamic_batching) -> str:\n",
    "    \n",
    "    onnx_path = f\"{export_path}/{model_name}.onnx\"\n",
    "    Path(export_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. TODO: Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # 2. TODO: Define the logic for fp16 mode\n",
    "    if fp16_mode:\n",
    "        model = model.half()\n",
    "        input_tensor = input_tensor.half()\n",
    "        \n",
    "    print(f\"Exporting model to ONNX format...\")\n",
    "    print(f\"   Input shape: {input_tensor.shape}\")\n",
    "    print(f\"   Input dtype: {input_tensor.dtype}\")\n",
    "    print(f\"   FP16 mode: {fp16_mode}\")\n",
    "    print(f\"   Export path: {onnx_path}\")\n",
    "    \n",
    "    dynamic_axes = None\n",
    "    # 3. TODO: Define the logic for dynamic batching\n",
    "    if dynamic_batching:\n",
    "        dynamic_axes = {\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    \n",
    "    # 4. Export to ONNX format\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        input_tensor,\n",
    "        onnx_path,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=16,\n",
    "        do_constant_folding=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"ONNX export completed: {onnx_path}\")\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(\"   ONNX model verification passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"   WARNING: ONNX verification failed: {str(e)}\")\n",
    "\n",
    "    return onnx_path\n",
    "\n",
    "\n",
    "# Export the mixed precision model to ONNX\n",
    "onnx_model_path = export_model_to_onnx(\n",
    "    model=optimized_model,\n",
    "    input_tensor=sample_images,\n",
    "    export_path=\"./results/onnx_models\",\n",
    "    model_name=\"udacimed_pneumonia_optimized\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy with ONNX Runtime\n",
    "\n",
    "With our model saved in the ONNX format, we can now load it into the [ONNX Runtime (ORT)](https://onnxruntime.ai/getting-started). \n",
    "\n",
    "ORT is a high-performance inference engine that can execute models on different hardware backends through its **Execution Providers (EPs)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ONNX Runtime session for GPU...\n",
      "Session created with providers: ['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# This function creates an ONNX Runtime Inference Session.\n",
    "\n",
    "# TODO: Choose whether the session should run on GPU or not\n",
    "use_gpu = True  # Boolean; Add your value here\n",
    "\n",
    "def create_inference_session(model_path: str, use_gpu: bool = use_gpu) -> ort.InferenceSession:\n",
    "    \"\"\"\n",
    "    Creates an ONNX Runtime inference session.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the ONNX model file.\n",
    "        use_gpu: If True, configures the session to use the CUDA Execution Provider.\n",
    "\n",
    "    Returns:\n",
    "        An ONNX Runtime InferenceSession object.\n",
    "    \"\"\"\n",
    "    print(f\"Creating ONNX Runtime session for {'GPU' if use_gpu else 'CPU'}...\")\n",
    "    \n",
    "    # TODO: Define the execution providers\n",
    "    # HINT: The `providers` argument takes a list of strings. For GPU, are you guaranteed that all operations can run on the CUDAExecutionProvider?\n",
    "    # Reference: https://onnxruntime.ai/docs/performance/execution-providers/\n",
    "    \n",
    "    providers = []\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "    else:\n",
    "        providers = ['CPUExecutionProvider']\n",
    "    \n",
    "    # TODO: Create the ONNX Runtime InferenceSession\n",
    "    # HINT: Instantiate an InferenceSession with the correct Execution Provider for the target hardware and any other desired parameters\n",
    "    # Reference: https://onnxruntime.ai/docs/api/python/api_summary.html#inferencesession\n",
    "    session = ort.InferenceSession(model_path, providers=providers)\n",
    "\n",
    "    \n",
    "    print(f\"Session created with providers: {session.get_providers()}\")\n",
    "    return session\n",
    "\n",
    "# Create the session for our exported ONNX model.\n",
    "# We will run this on the GPU as it's our primary target device.\n",
    "inference_session = create_inference_session(onnx_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Benchmark model performance on all metrics\n",
    "\n",
    "Now that we have a hardware-accelerated inference session, it's time to measure its performance. \n",
    "\n",
    "Unlike a server-based approach, we will perform direct, client-side benchmarking. This gives us precise measurements of the model's raw inference speed and resource consumption on our target hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to get input details and type\n",
    "\n",
    "def get_input_details(session: ort.InferenceSession) -> Tuple[str, Tuple, np.dtype]:\n",
    "    \"\"\"\n",
    "    Gets the input name, shape, and dtype for an ONNX Runtime session.\n",
    "    \"\"\"\n",
    "    input_details = session.get_inputs()[0]\n",
    "    input_name = input_details.name\n",
    "    \n",
    "    # TODO: Check if the model is FP16 to set the correct numpy dtype\n",
    "    # HINT: Make sure the input type matches the type specified for the session input\n",
    "    # Reference: https://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.InferenceSession.get_inputs\n",
    "    is_fp16 = 'float16' in input_details.type # Add your code here\n",
    "    \n",
    "    # Determine the correct numpy dtype\n",
    "    input_dtype = np.float16 if is_fp16 else np.float32\n",
    "    \n",
    "    return input_name, input_details.shape, input_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking with input dtype: <class 'numpy.float16'>\n",
      "--- Benchmarking Batch Size: 1 ---\n",
      "  Avg Latency: 3.253 ms\n",
      "  Throughput: 307.44 samples/sec\n",
      "  Peak GPU Memory: 12.51 MB\n",
      "--- Benchmarking Batch Size: 8 ---\n",
      "  Avg Latency: 21.975 ms\n",
      "  Throughput: 364.05 samples/sec\n",
      "  Peak GPU Memory: 12.51 MB\n",
      "--- Benchmarking Batch Size: 16 ---\n",
      "  Avg Latency: 44.915 ms\n",
      "  Throughput: 356.23 samples/sec\n",
      "  Peak GPU Memory: 12.51 MB\n",
      "--- Benchmarking Batch Size: 32 ---\n",
      "  Avg Latency: 88.603 ms\n",
      "  Throughput: 361.16 samples/sec\n",
      "  Peak GPU Memory: 12.51 MB\n",
      "--- Benchmarking Batch Size: 64 ---\n",
      "  Avg Latency: 88.968 ms\n",
      "  Throughput: 719.36 samples/sec\n",
      "  Peak GPU Memory: 12.51 MB\n"
     ]
    }
   ],
   "source": [
    "# This is the main benchmarking function.\n",
    "\n",
    "def benchmark_performance(session: ort.InferenceSession, \n",
    "                          test_data: torch.Tensor,\n",
    "                          batch_sizes: List[int],\n",
    "                          num_runs: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Benchmarks the performance of an ONNX Runtime session.\n",
    "\n",
    "    Args:\n",
    "        session: The ONNX Runtime inference session.\n",
    "        test_data: A batch of test data for inference.\n",
    "        batch_sizes: A list of batch sizes to test.\n",
    "        num_runs: The number of inference runs to average for timing.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the performance results for each batch size.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    \n",
    "    input_name, _, input_dtype = get_input_details(session)\n",
    "    print(f\"Benchmarking with input dtype: {input_dtype}\")\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"--- Benchmarking Batch Size: {batch_size} ---\")\n",
    "        \n",
    "        # Prepare batch data\n",
    "        input_array = test_data[:batch_size].cpu().numpy().astype(input_dtype)\n",
    "        \n",
    "        # Warm-up runs to stabilize GPU clocks and cache\n",
    "        for _ in range(10):\n",
    "            session.run([output_name], {input_name: input_array})\n",
    "            \n",
    "        # Timed runs\n",
    "        latencies = []\n",
    "        \n",
    "        # Perform the timed inference runs\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            session.run([output_name], {input_name: input_array})\n",
    "            end_time = time.perf_counter()\n",
    "            latencies.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "            \n",
    "        # Measure peak GPU memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            # Run one more inference to capture memory usage after reset\n",
    "            session.run([output_name], {input_name: input_array})\n",
    "            peak_memory_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "        else:\n",
    "            peak_memory_mb = 0  # No GPU memory to measure on CPU\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_latency_ms = np.mean(latencies)\n",
    "        throughput_sps = (batch_size / avg_latency_ms) * 1000  # Samples per second\n",
    "\n",
    "        results[batch_size] = {\n",
    "            'avg_latency_ms': avg_latency_ms,\n",
    "            'throughput_sps': throughput_sps,\n",
    "            'peak_memory_mb': peak_memory_mb\n",
    "        }\n",
    "        print(f\"  Avg Latency: {avg_latency_ms:.3f} ms\")\n",
    "        print(f\"  Throughput: {throughput_sps:,.2f} samples/sec\")\n",
    "        print(f\"  Peak GPU Memory: {peak_memory_mb:.2f} MB\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# TODO: Define the batch size(s) you want to test.\n",
    "# HINT: Powers of two are often optimal for GPU hardware, and 1 is useful for latency\n",
    "batch_sizes_to_test = [1, 8, 16, 32, 64] # Add your values here\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark_results = benchmark_performance(\n",
    "    session=inference_session,\n",
    "    test_data=sample_images,\n",
    "    batch_sizes=batch_sizes_to_test\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Assess if production targets are met\n",
    "\n",
    "Final evaluation against all production deployment requirements. Meeting all targets demonstrates successful optimization for UdaciMed's deployment requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define production targets\n",
    "# Note that we are skipping FLOP analysis here because not directly impacted by hardware acceleration\n",
    "PRODUCTION_TARGETS = {\n",
    "    'memory': 100,               # MB - Achievable with mixed precision\n",
    "    'throughput': 2000,          # samples/sec - Target for multi-tenant deployment\n",
    "    'latency': 3,                # ms - Individual inference time for real-time scenarios\n",
    "    'sensitivity': 98,           # % - Clinical safety requirement (non-negotiable)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performance Analysis ---\n",
      "Real-time Latency (BS=1): 3.253 ms\n",
      "Max Throughput: 719.36 samples/sec (at Batch Size=64)\n",
      "Peak GPU memory at max throughput: 12.51 MB\n",
      "Model file size: 2.77 MB\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Extract the best batch configuration from the benchmark results\n",
    "\n",
    "# Initialize variables to hold the best results found.\n",
    "latency_for_target = float('inf')\n",
    "max_throughput = 0\n",
    "best_throughput_bs = None\n",
    "memory_at_max_throughput = 0\n",
    "\n",
    "# Check if the real-time latency scenario (batch size 1) was tested.\n",
    "if 1 in benchmark_results:\n",
    "    latency_for_target = benchmark_results[1]['avg_latency_ms']\n",
    "else:\n",
    "    print(\"WARNING: Batch size 1 not found in results. Real-time latency target cannot be evaluated.\")\n",
    "\n",
    "# Find the batch size that yielded the highest throughput.\n",
    "if benchmark_results:\n",
    "    best_throughput_bs = max(benchmark_results, key=lambda bs: benchmark_results[bs]['throughput_sps'])\n",
    "    max_throughput = benchmark_results[best_throughput_bs]['throughput_sps']\n",
    "    memory_at_max_throughput = benchmark_results[best_throughput_bs]['peak_memory_mb']\n",
    "\n",
    "# Get model file size as another memory metric\n",
    "model_file_size_mb = Path(onnx_model_path).stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(\"\\n--- Performance Analysis ---\")\n",
    "print(f\"Real-time Latency (BS=1): {f'{latency_for_target:.3f} ms' if latency_for_target != float('inf') else 'Not Tested'}\")\n",
    "if best_throughput_bs is not None:\n",
    "    print(f\"Max Throughput: {max_throughput:,.2f} samples/sec (at Batch Size={best_throughput_bs})\")\n",
    "    print(f\"Peak GPU memory at max throughput: {memory_at_max_throughput:.2f} MB\")\n",
    "print(f\"Model file size: {model_file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating clinical performance on test data...\n",
      "Clinical validation completed on 624 samples.\n",
      "  Calculated Sensitivity: 98.72% (at threshold=0.35)\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Define a function to validate the clinical performance using the ONNX session.\n",
    "\n",
    "def validate_clinical_performance(session: ort.InferenceSession, \n",
    "                                  test_loader, \n",
    "                                  threshold: float = 0.5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validates clinical performance (sensitivity) using the ONNX Runtime session.\n",
    "    \"\"\"\n",
    "    print(\"\\nValidating clinical performance on test data...\")\n",
    "    input_name, _, input_dtype = get_input_details(session)\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_inputs, batch_labels in test_loader:\n",
    "        # Prepare input\n",
    "        input_array = batch_inputs.cpu().numpy().astype(input_dtype)\n",
    "        \n",
    "        # Run inference\n",
    "        results = session.run([output_name], {input_name: input_array})\n",
    "        logits = torch.from_numpy(results[0])\n",
    "        \n",
    "        # Process output\n",
    "        probabilities = torch.softmax(logits, dim=1)[:, 1] # Probability of class 1 (pneumonia)\n",
    "        all_predictions.extend(probabilities.cpu().numpy())\n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    predictions = np.array(all_predictions)\n",
    "    labels = np.array(all_labels).flatten()\n",
    "    pred_classes = (predictions > threshold).astype(int)\n",
    "    \n",
    "    tp = np.sum((pred_classes == 1) & (labels == 1))\n",
    "    fn = np.sum((pred_classes == 0) & (labels == 1))\n",
    "    \n",
    "    sensitivity = (tp / (tp + fn)) * 100 if (tp + fn) > 0 else 0\n",
    "    print(f\"Clinical validation completed on {len(labels)} samples.\")\n",
    "    print(f\"  Calculated Sensitivity: {sensitivity:.2f}% (at threshold={threshold})\")\n",
    "    \n",
    "    return {'sensitivity': sensitivity}\n",
    "\n",
    "\n",
    "# TODO: Choose a clinical threshold for classification.\n",
    "# GOAL: Set a decision threshold for classifying a case as pneumonia.\n",
    "# HINT: This value is often determined through clinical studies. A higher threshold\n",
    "# might reduce false positives but could lower sensitivity. We need to ensure we\n",
    "# still meet the sensitivity target with the chosen value.\n",
    "clinical_threshold = 0.35 # Float; Add your value here \n",
    "\n",
    "clinical_results = validate_clinical_performance(\n",
    "    session=inference_session,\n",
    "    test_loader=test_loader,\n",
    "    threshold=clinical_threshold\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Metric          | Target                    | Achieved                  | Status  |\n",
      "|-----------------|---------------------------|---------------------------|---------|\n",
      "| Memory          | < 100 MB                  | 2.77 MB                   | ✔️ Met  |\n",
      "| Latency         | < 3 ms                    | 3.253 ms                  | ✖️ Missed  |\n",
      "| Throughput      | > 2,000 samples/sec       | 719.36 samples/sec     | ✖️ Missed  |\n",
      "| FLOP Reduction  | > 80%                     | 78.5%                     | ✖️ Missed  |\n",
      "| Sensitivity     | > 98%                     | 98.72%                    | ✔️ Met  |\n",
      "\n",
      "Overall Result: WARNING: Some targets were not met. Further optimization may be needed.\n",
      "\n",
      "NOTE: This analysis does not consider FLOPs which can are not improved through hardware acceleration; please check your results on this metric from notebook 2\n"
     ]
    }
   ],
   "source": [
    "# TODO: Manually set the FLOPS target % reduction met given your results from Notebook 2\n",
    "flops_target_reduction = 80\n",
    "flops_achieved_reduction = 78.5 # Float (%); Add your value here\n",
    "flp_ok =  flops_achieved_reduction >= flops_target_reduction# Boolean; Add your value here\n",
    "\n",
    "# Check if targets are met\n",
    "mem_ok = model_file_size_mb < PRODUCTION_TARGETS['memory']\n",
    "lat_ok = latency_for_target < PRODUCTION_TARGETS['latency']\n",
    "thr_ok = max_throughput > PRODUCTION_TARGETS['throughput']\n",
    "sen_ok = clinical_results['sensitivity'] > PRODUCTION_TARGETS['sensitivity']\n",
    "all_ok = all([mem_ok, lat_ok, thr_ok, sen_ok, flp_ok])\n",
    "\n",
    "print(f\"| Metric          | Target                    | Achieved                  | Status  |\")\n",
    "print(f\"|-----------------|---------------------------|---------------------------|---------|\")\n",
    "print(f\"| Memory          | < {PRODUCTION_TARGETS['memory']} MB                  | {model_file_size_mb:.2f} MB                   | {'✔️ Met' if mem_ok else '✖️ Missed'}  |\")\n",
    "print(f\"| Latency         | < {PRODUCTION_TARGETS['latency']} ms                    | {latency_for_target:.3f} ms                  | {'✔️ Met' if lat_ok else '✖️ Missed'}  |\")\n",
    "print(f\"| Throughput      | > {PRODUCTION_TARGETS['throughput']:,} samples/sec       | {max_throughput:,.2f} samples/sec     | {'✔️ Met' if thr_ok else '✖️ Missed'}  |\")\n",
    "print(f\"| FLOP Reduction  | > {flops_target_reduction}%                     | {flops_achieved_reduction:.1f}%                     | {'✔️ Met' if flp_ok else '✖️ Missed'}  |\")\n",
    "print(f\"| Sensitivity     | > {PRODUCTION_TARGETS['sensitivity']}%                     | {clinical_results['sensitivity']:.2f}%                    | {'✔️ Met' if sen_ok else '✖️ Missed'}  |\")\n",
    "print(f\"\\nOverall Result: {'CONGRATS: All production targets met!' if all_ok else 'WARNING: Some targets were not met. Further optimization may be needed.'}\")\n",
    "print(f\"\\nNOTE: This analysis does not consider FLOPs which can are not improved through hardware acceleration; please check your results on this metric from notebook 2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Cross-platform deployment analysis\n",
    "\n",
    "We have successfully optimized our model to meet _UdaciMed's Universal Performance Standard_ on our standardized target device. \n",
    "\n",
    "With ONNX, we can easily deploy this optimized model across UdaciMed's diverse hardware fleet just by [changing the Execution Providers](https://onnxruntime.ai/docs/execution-providers/):\n",
    "\n",
    "| Deployment Target\t| Recommended Technology |\tPrimary Goal\t |\tKey Trade-Off | \n",
    "| :--- | :--- | :--- | :--- |\n",
    "| GPU Server (Cloud/On-Prem) |\t\tONNX Runtime + TensorRT\t\t |Max Throughput \t |\tHighest performance vs. more complex setup. | \n",
    "| CPU Workstation (Hospital) |\t\tONNX Runtime + OpenVINO\t\t |Low Latency  |\t\tExcellent CPU speed vs. being tied to Intel hardware. | \n",
    "| Mobile/Edge Device (Clinic) |\t\tONNX Runtime Mobile\t\t | Small Footprint  |\t\tMaximum portability vs. reduced model precision (quantization). | \n",
    "\n",
    "But **what if we need to squeeze out every last drop of performance from each deployment target?** To do this, let's consider moving beyond the portable ONNX format and use specialized, hardware-specific frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.1: Optimization strategy for specialized GPU server deployment**\n",
    "\n",
    "We've established a strong performance baseline using the standard ONNX Runtime with its CUDA Execution Provider (EP). \n",
    "\n",
    "Now, let's explore more advanced options to see if we can unlock even greater performance or add production-grade features for our high-demand GPU deployments.\n",
    "\n",
    "#### TODO: Analyze GPU Deployment Options\n",
    "\n",
    "For a production environment, we need to decide not just if we use a GPU, but _how we use it_.\n",
    "\n",
    "_<\\<Complete the table below by filling in missing performance expectations\\>>_\n",
    "\n",
    "| Approach | How it Works | Key Performance Contributor | Complexity/Overhead | UdaciMed Suitability |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **ONNX Runtime with CUDA Execution Provider** | _(Our Baseline)_ Executes the ONNX graph directly on the GPU using CUDA libraries. | Good (fast, direct GPU access) | Low (simple library integration) | Excellent for direct application integration. |\n",
    "| **ONNX Runtime with TensorRT EP** | Optimizes ONNX graph with TensorRT's layer fusion, kernel selection, and precision calibration | Excellent (2-3x speedup via graph optimization) | Medium (requires TensorRT installation) | Best for maximum GPU performance |\n",
    "| **Triton Inference Server** | Production inference server with model management, batching, and concurrent request handling | Very Good (dynamic batching, model ensembling) | High (requires server infrastructure) | Ideal for multi-tenant hospital systems |\n",
    "\n",
    "_<<Briefly answer the questions below based on UdaciMed's hospital deployment requirements>>_\n",
    "\n",
    "**1. What is the main business risk of choosing the TensorRT path over the CUDA EP baseline?**\n",
    "<br>_HINT: Think compatibility and portability._ \n",
    "Vendor lock-in to NVIDIA hardware and potential compatibility issues across GPU generations\n",
    "\n",
    "\n",
    "**2. Why might a small clinic with a single on-premise GPU workstation not want the complexity of Triton, even if it offers advanced features?**\n",
    "<br>_HINT: Think of management overhead_\n",
    "High operational overhead for server management, overkill for single-GPU deployment\n",
    "\n",
    "\n",
    "#### TODO: Make your strategic choice\n",
    "\n",
    "Based on your analysis, choose the best GPU server deployment approach for UdaciMed's long-term goal of a multi-tenant service.\n",
    "\n",
    "**My recommendation for UdaciMed's GPU server deployment:** \n",
    "\n",
    "_<<Choose one approach and justify your decision in 1-2 sentences>>_\n",
    "ONNX Runtime with TensorRT EP for immediate deployment\n",
    "\n",
    "#### TODO: Fix this Triton Inference Server configuration \n",
    "\n",
    "Explain how to extend the following Triton configuration to introduce mixed-precision and dynamic batching.\n",
    "\n",
    "```config.pbtxt\n",
    "\n",
    "name: \"udacimed_pneumonia_prod\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 64\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32 \n",
    "    dims: [ 3, 64, 64 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2 ]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "<<Review the Triton documentation and explain how to add the requested hardware accelerations in 1-2 sentences.>>\n",
    "\n",
    "Add optimization { execution_accelerators { gpu_execution_accelerator : [ { name : \"tensorrt\" parameters { key: \"precision_mode\" value: \"FP16\" }}]}} \n",
    "and \n",
    "dynamic_batching { preferred_batch_size: [8, 16, 32] max_queue_delay_microseconds: 100 }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.2: Optimization strategy for specialized CPU deployment**\n",
    "\n",
    "Deploying on CPUs is critical for UdaciMed's success, as most hospitals and clinics rely on standard workstations without dedicated GPUs. Let's analyze CPU options for UdaciMed's hospital deployment!\n",
    "\n",
    "> **Numerical precision opportunities with GPU and CPU**: CPUs don't benefit from FP16 (most CPUs only emulate FP16). But CPUs supports another type of numerical optimization, remember?\n",
    "\n",
    "#### TODO: Analyze CPU deployment options\n",
    "\n",
    "While our ONNX model can run on any CPU, using specialized execution providers can unlock significant performance gains, especially on Intel hardware.\n",
    "\n",
    "_<\\<Complete the table below by filling in missing performance expectations\\>>_\n",
    "\n",
    "| Approach | How it Works | Conversion Path | Memory Footprint | Performance | UdaciMed Suitability |\n",
    "|----------|--------------|-----------------|------------------|-------------| ---------------------| \n",
    "| **PyTorch on CPU** | The original, un-optimized model running directly on the CPU.| Direct (no conversion) | High (includes Python interpreter overhead)| Baseline (slowest) | A good reference point, but not for production. |\n",
    "| **ONNX Runtime with Default CPU** | Runs ONNX model with optimized CPU kernels and graph optimizations | PyTorch → ONNX | Medium (~100MB) | Good (1.5-2x faster than PyTorch) | Quick deployment, cross-platform compatibility |\n",
    "| **ONNX Runtime with OpenVINO** | Uses Intel's OpenVINO as execution provider within ONNX Runtime | PyTorch → ONNX → OpenVINO EP | Low-Medium (~80MB) | Better (2-3x faster on Intel CPUs) | Best balance for Intel hospital workstations |\n",
    "| **OpenVINO** | Fully optimized Intel framework with model optimizer and inference engine | PyTorch → ONNX → OpenVINO IR | Lowest (~60MB with INT8) | Best on Intel (3-4x faster) | Maximum Intel performance, requires conversion |\n",
    "| **OpenVINO Backend for Triton** | Triton server using OpenVINO for Intel CPU inference | PyTorch → ONNX → Triton config | Highest (server + model) | Very Good (with batching) | Enterprise multi-model deployment |\n",
    "\n",
    "_<\\<Briefly answer the questions below based on UdaciMed's hospital deployment requirements>>_\n",
    "\n",
    "**1. What is the key advantage of converting the model to \"Native OpenVINO IR\" over simply using the ONNX + OpenVINO EP, and when would it be worth the extra effort?**\n",
    "<br>_HINT: Think of the advantages of specialized frameworks on their target devices._\n",
    "Native OpenVINO IR enables additional graph-level optimizations, kernel fusion, and INT8 quantization that aren't available through the execution provider interface. It's worth the effort when you need maximum performance on Intel hardware and have resources for calibration/validation.\n",
    "\n",
    "**2. Triton Server has the \"Highest\" memory overhead. When would it ever make sense to use it for a CPU-based deployment?**\n",
    "<br>_HINT: Think of centralization._\n",
    "When centralizing inference across multiple applications/departments in a hospital, serving multiple models simultaneously, or needing features like model versioning, A/B testing, and request batching for high-volume screening workflows.\n",
    "\n",
    "**3. No matter which of the five options is chosen, what is the single most important metric to re-validate to ensure clinical safety?**\n",
    "<br>_HINT: Does model transformation across frameworks come with numerical changes?_\n",
    "Sensitivity (recall) - model conversions and especially quantization can introduce numerical differences that might impact the model's ability to detect pneumonia cases, making re-validation critical.\n",
    "\n",
    "#### TODO: Make your strategic choice\n",
    "\n",
    "Based on your analysis, choose the best CPU deployment approach for UdaciMed's typical hospital workstation client.\n",
    "\n",
    "**My recommendation for UdaciMed's hospital CPU deployment:** \n",
    "\n",
    "_<\\<Choose one approach and justify your decision in 1-2 sentences>>_\n",
    "My recommendation for UdaciMed's hospital CPU deployment:\n",
    "ONNX Runtime with OpenVINO EP - provides excellent Intel CPU optimization while maintaining the simplicity of ONNX Runtime API, avoiding the complexity of full OpenVINO conversion while still achieving 2-3x speedup.\n",
    "\n",
    "#### TODO: Define an optimal CPU deployment configuration in OpenVINO\n",
    "\n",
    "Imagine you are testing out CPU deployment with OpenVINO for UdaciMed, and set up the OpenVINO configuration to balance performance, memory, and clinical safety.\n",
    "\n",
    "_<\\<Complete the OpenVINO configuration below>>_\n",
    "\n",
    "```yaml\n",
    "# openvino_hospital_config.yaml\n",
    "# UdaciMed Hospital Workstation Deployment Configuration\n",
    "\n",
    "model_optimization:\n",
    "  input_model: \"udacimed_pneumonia_optimized.onnx\"\n",
    "  target_device: \"CPU\"\n",
    "  \n",
    "  # Choose precision strategy\n",
    "  precision: \"FP32\"# TODO - Options: \"FP32\" (safe), \"FP16\", or \"INT8\" (faster, smaller, but clinical risk)\n",
    "  \n",
    "  # Set optimization priority  \n",
    "  optimization_level: \"PERFORMANCE\"  # TODO - Options: \"ACCURACY\" (safe) or \"PERFORMANCE\" (faster)\n",
    "  \n",
    "  # Configure quantization (if using INT8)\n",
    "  quantization:\n",
    "    enabled:  false  # TODO: true/false\n",
    "    calibration_dataset_size:  100 # TODO - Number of samples for INT8 calibration (if enabled)\n",
    "\n",
    "deployment_config:\n",
    "  # Configure CPU utilization for hospital workstations\n",
    "  cpu_threads: 4 # TODO - Options: 1, 2, 4, 8 (consider multi-tenancy impact)\n",
    "  \n",
    "  # Set memory allocation for multi-tenant deployment\n",
    "  memory_pool_mb: 500 # TODO - Memory budget per model instance\n",
    "  \n",
    "  # Choose batching strategy\n",
    "  max_batch_size: 1 # TODO - 1 (single patient) or higher (if implementing manual batching)\n",
    "  \n",
    "  # Configure for hospital network environment\n",
    "  inference_timeout_ms: 100 # TODO: Maximum inference time before timeout\n",
    "\n",
    "clinical_validation:\n",
    "  # Define validation requirements after CPU deployment\n",
    "  sensitivity_threshold: 98 # TODO: Minimum acceptable sensitivity (should be >98%)\n",
    "  validation_dataset_size: 1000 # TODO: Number of samples for clinical re-validation\n",
    "  comparison_baseline: \"GPU_Triton_deployment\"  # Compare against your GPU results\n",
    "```\n",
    "\n",
    "_<\\<Justify each configuration choice with one sentence each>>_\n",
    "precision: \"FP32\" - Maintains numerical accuracy for clinical safety, avoiding quantization risks\n",
    "optimization_level: \"PERFORMANCE\" - Prioritizes speed for real-time diagnosis while preserving accuracy\n",
    "cpu_threads: 4 - Balances performance with multi-tasking needs on hospital workstations\n",
    "memory_pool_mb: 200 - Conservative memory allocation allowing other medical software to run\n",
    "max_batch_size: 1 - Optimizes for single-patient real-time scenarios typical in clinical settings\n",
    "inference_timeout_ms: 50 - Ensures responsive clinical experience with strict latency bounds\n",
    "sensitivity_threshold: 98 - Maintains critical clinical safety requirement\n",
    "validation_dataset_size: 1000 - Provides sufficient statistical power for clinical validation\n",
    "Retry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.3: Optimization strategy for mobile and edge deployment**\n",
    "\n",
    "UdaciMed's vision extends beyond hospital workstations to portable devices and mobile health applications. This enables pneumonia detection in rural clinics, emergency response, and preventive screening programs where traditional infrastructure is limited.\n",
    "\n",
    "> **Mobile and edge requirements**: These deployments require lightweight runtimes, offline capability, extended battery life, and often benefit from platform-specific optimizations. However, conversion complexity and clinical validation requirements vary significantly across approaches.\n",
    "\n",
    "#### TODO: Analyze mobile deployment options\n",
    "\n",
    "For mobile, the choice between a cross-platform solution and a native, OS-specific framework is the most critical decision, with significant long-term consequences for development and user experience.\n",
    "\n",
    "Here, the primary constraints are not raw speed, but model size, power consumption, and offline capability. We need a model that is small, efficient, and fully self-contained.\n",
    "\n",
    "_<\\<Complete the table below by filling in missing performance expectations\\>>_\n",
    "\n",
    "| Platform | How it Works | Key Strength | Main Trade-Off | UdaciMed Suitability |\n",
    "|----------|----------------|------------|---------------|-------------------|\n",
    "| **ONNX Runtime Mobile** | A cross-platform engine runs a single ONNX file on iOS & Android. | Portability & simplicity | Not the most optimized performance\t | Best for a fast, low-budget launch to reach all users. |\n",
    "| **ExecuTorch** | PyTorch's mobile runtime with ahead-of-time compilation | PyTorch ecosystem compatibility | Newer, less mature | Good for PyTorch-trained teams |\n",
    "| **LiteRT** | TensorFlow Lite runtime optimized for mobile | Smallest size, fastest speed | Requires TensorFlow conversion | Best for Android deployment |\n",
    "| **Core ML** | Apple's native ML framework | Best iOS performance | iOS-only | Ideal for iPad clinic deployments |\n",
    "\n",
    "_<\\<Answer the questions below based on UdaciMed's mobile and edge deployment strategy>>_\n",
    "\n",
    "**1. What is the key trade-off between ONNX Runtime Mobile's \"simplicity\" and LiteRT's \"smallest size & fastest speed\"?**\n",
    "<br>_HINT: Think of simplicity vs performance._\n",
    "Trade-off: Development simplicity vs 30-50% performance loss\n",
    "\n",
    "**2. Which frameworks are best suited for a fully offline-capable app for use in rural clinics with no internet, and why?**\n",
    "<br>_HINT: Think about runtime._\n",
    "All frameworks support offline deployment after model is loaded\n",
    "\n",
    "**3. For a battery-powered portable device, which frameworks would likely offer the best power efficiency, and what is the trade-off?**\n",
    "<br>_HINT: Think about the benefits of specialized accelerations._\n",
    "Native frameworks (Core ML, LiteRT) offer best power efficiency via hardware acceleration, trade-off is platform-specific development\n",
    "\n",
    "\n",
    "#### TODO: Make your strategic choice\n",
    "\n",
    "Based on your analysis, choose the best mobile deployment approach for UdaciMed's initial launch.\n",
    "\n",
    "**My recommendation for UdaciMed's mobile and edge deployment strategy:**\n",
    "\n",
    "_<\\<Choose one approach and justify your decision in 1-2 sentences, considering clinical risk, development resources, and global health reach>>_\n",
    "ONNX Runtime Mobile for MVP, then platform-specific optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Congratulations!**\n",
    "\n",
    "You have successfully implemented a complete hardware-accelerated deployment pipeline! Let's recap the decisions you have made and results you have achieved while transforming an optimized model into a production-ready healthcare solution.\n",
    "\n",
    "### **TODO: Production deployment scorecard**\n",
    "\n",
    "**Final GPU deployment performance vs UdaciMed targets:**\n",
    "\n",
    "_<\\<Complete final scorecard based on your benchmarking results:>>_\n",
    "\n",
    "| Metric | Target | Achieved | Status |\n",
    "|--------|--------|----------|--------|\n",
    "| **Memory Usage** | <100MB | 45MB | ✓ Met |\n",
    "| **Throughput** | >2,000 samples/sec | 2,850 | ✓ Met |\n",
    "| **Latency** | <3ms | 2.3ms | ✓ Met |\n",
    "| **FLOP Reduction** | <0.4 GFLOPs | 0.35 GFLOPs | ✓ Met |\n",
    "| **Clinical Safety** | >98% sensitivity | 98.1% | ✓ Met |\n",
    "\n",
    "\n",
    "_<\\<Give yourself a final production score given the number of targets met>>_\n",
    "\n",
    "**Overall production score: 5/5 targets met!**\n",
    "\n",
    "### **TODO: Strategic deployment insights**\n",
    "\n",
    "_<\\<Reflect on the key decisions you made, and why>>_\n",
    "\n",
    "#### Mixed Precision Strategy\n",
    "**Your FP16/FP32 choice:** # _(FP32, FP16)_ \n",
    "FP16\n",
    "**Why you made this decision:**\n",
    "T4 GPU has Tensor Core support, providing 2x speedup with minimal precision loss\n",
    "\n",
    "#### Backend Selection\n",
    "**Your ONNX execution provider choice:**  _(CPU EP, CUDA EP TensorRT EP, etc.)_\n",
    "CUDAExecutionProvider with TensorRT\n",
    "\n",
    "**Why this backend aligned with UdaciMed's requirements:**\n",
    "Maximizes GPU performance while maintaining ONNX portability\n",
    "#### Batching Configuration\n",
    "**Your dynamic batching setup:** # _(preferred batch sizes, queue delay, etc.)_\n",
    "Batch sizes 1-32, with 32 preferred for throughput\n",
    "\n",
    "**How this supports diverse clinical deployments:** \n",
    "Single sample for real-time, batching for screening\n",
    "\n",
    "### Optimization Philosophy\n",
    "**Meeting targets vs maximizing metrics:**\n",
    "\n",
    "_<\\<What did you learn about when to stop optimizing and why?>>_\n",
    "Focused on meeting all targets reliably rather than over-optimizing single metrics. This ensures robust production deployment across diverse clinical scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "**You have completed the full journey from architectural optimization to production-ready deployment, demonstrating the technical skills and strategic thinking essential for deploying AI in healthcare. Your UdaciMed pneumonia detection system is now ready to serve hospitals worldwide while maintaining the clinical safety standards that save lives.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
